/**
 * Client Claude optimis√© avec Prompt Caching
 *
 * √âconomies attendues :
 * - Prompt Caching : -90% sur tokens input r√©p√©t√©s
 * - Optimisation prompts : -20% sur tokens output
 * - Total : ~50% de r√©duction de co√ªts
 */

import Anthropic from '@anthropic-ai/sdk'
import { SECURITY_LIMITS } from '@/lib/api-security'

// Cache singleton pour √©viter multiples initialisations
let anthropicClient: Anthropic | null = null

/**
 * R√©cup√®re le client Anthropic (singleton)
 */
export function getOptimizedAnthropicClient(): Anthropic {
  if (!anthropicClient) {
    if (!process.env.ANTHROPIC_API_KEY) {
      throw new Error('ANTHROPIC_API_KEY is not configured')
    }

    anthropicClient = new Anthropic({
      apiKey: process.env.ANTHROPIC_API_KEY,
      timeout: SECURITY_LIMITS.CLAUDE_TIMEOUT_MS
    })
  }

  return anthropicClient
}

/**
 * Interface pour les m√©triques de g√©n√©ration
 */
export interface GenerationMetrics {
  inputTokens: number
  outputTokens: number
  cacheCreationTokens?: number
  cacheReadTokens?: number
  totalCost: number
  estimatedSavings?: number
}

/**
 * Calcule le co√ªt d'une g√©n√©ration Claude
 */
export function calculateClaudeCost(
  inputTokens: number,
  outputTokens: number,
  cacheCreationTokens: number = 0,
  cacheReadTokens: number = 0
): { cost: number; savings: number } {
  // Tarifs Claude 3.5 Sonnet (janvier 2026)
  const INPUT_COST_PER_1M = 3.00      // $3 / 1M tokens
  const OUTPUT_COST_PER_1M = 15.00    // $15 / 1M tokens
  const CACHE_WRITE_COST_PER_1M = 3.75 // $3.75 / 1M tokens (25% markup)
  const CACHE_READ_COST_PER_1M = 0.30  // $0.30 / 1M tokens (90% discount)

  // Co√ªt sans cache
  const costWithoutCache =
    (inputTokens * INPUT_COST_PER_1M / 1_000_000) +
    (outputTokens * OUTPUT_COST_PER_1M / 1_000_000)

  // Co√ªt avec cache
  const costWithCache =
    (cacheCreationTokens * CACHE_WRITE_COST_PER_1M / 1_000_000) +
    (cacheReadTokens * CACHE_READ_COST_PER_1M / 1_000_000) +
    ((inputTokens - cacheCreationTokens - cacheReadTokens) * INPUT_COST_PER_1M / 1_000_000) +
    (outputTokens * OUTPUT_COST_PER_1M / 1_000_000)

  const savings = costWithoutCache - costWithCache

  return {
    cost: costWithCache,
    savings
  }
}

/**
 * G√©n√®re avec Claude en utilisant le Prompt Caching
 *
 * @param systemPrompt - Prompt syst√®me (sera cach√© automatiquement)
 * @param userPrompt - Prompt utilisateur (dynamique)
 * @param options - Options additionnelles
 */
export async function generateWithCaching(
  systemPrompt: string,
  userPrompt: string,
  options: {
    model?: string
    maxTokens?: number
    temperature?: number
  } = {}
): Promise<{
  content: string
  metrics: GenerationMetrics
}> {
  const client = getOptimizedAnthropicClient()

  const response = await client.messages.create({
    model: options.model || 'claude-3-5-sonnet-20241022',
    max_tokens: options.maxTokens || 2048,
    temperature: options.temperature || 1.0,

    // üî• PROMPT CACHING : Le system prompt sera cach√©
    system: [
      {
        type: 'text',
        text: systemPrompt,
        // Cette annotation indique √† Claude de cacher ce bloc
        cache_control: { type: 'ephemeral' }
      }
    ],

    messages: [
      {
        role: 'user',
        content: userPrompt
      }
    ]
  })

  // Extraire les m√©triques
  const usage = response.usage
  const metrics: GenerationMetrics = {
    inputTokens: usage.input_tokens,
    outputTokens: usage.output_tokens,
    cacheCreationTokens: usage.cache_creation_input_tokens || 0,
    cacheReadTokens: usage.cache_read_input_tokens || 0,
    totalCost: 0,
    estimatedSavings: 0
  }

  // Calculer le co√ªt et √©conomies
  const { cost, savings } = calculateClaudeCost(
    metrics.inputTokens,
    metrics.outputTokens,
    metrics.cacheCreationTokens,
    metrics.cacheReadTokens
  )

  metrics.totalCost = cost
  metrics.estimatedSavings = savings

  // Extraire le contenu
  const content = response.content[0].type === 'text'
    ? response.content[0].text
    : ''

  return { content, metrics }
}

/**
 * G√©n√®re plusieurs items en streaming optimis√©
 * (Pas de batching r√©el car g√©n√©ration s√©quentielle n√©cessaire)
 */
export async function generateMultipleWithCaching(
  systemPrompt: string,
  userPrompts: string[],
  options: {
    model?: string
    maxTokens?: number
    temperature?: number
  } = {}
): Promise<Array<{
  content: string
  metrics: GenerationMetrics
}>> {
  const results = []

  // G√©n√©rer s√©quentiellement pour b√©n√©ficier du cache
  for (const userPrompt of userPrompts) {
    const result = await generateWithCaching(systemPrompt, userPrompt, options)
    results.push(result)

    // Petit d√©lai pour √©viter rate limiting
    await new Promise(resolve => setTimeout(resolve, 100))
  }

  return results
}

/**
 * Optimise un prompt en supprimant la verbosit√© inutile
 */
export function optimizePrompt(prompt: string): string {
  return prompt
    // Supprimer espaces multiples
    .replace(/\s+/g, ' ')
    // Supprimer lignes vides multiples
    .replace(/\n\s*\n\s*\n/g, '\n\n')
    // Trim
    .trim()
}

/**
 * Construit les m√©triques agr√©g√©es
 */
export function aggregateMetrics(metrics: GenerationMetrics[]): {
  totalInputTokens: number
  totalOutputTokens: number
  totalCacheReadTokens: number
  totalCost: number
  totalSavings: number
  averageCostPerGeneration: number
} {
  const totals = metrics.reduce(
    (acc, m) => ({
      inputTokens: acc.inputTokens + m.inputTokens,
      outputTokens: acc.outputTokens + m.outputTokens,
      cacheReadTokens: acc.cacheReadTokens + (m.cacheReadTokens || 0),
      cost: acc.cost + m.totalCost,
      savings: acc.savings + (m.estimatedSavings || 0)
    }),
    { inputTokens: 0, outputTokens: 0, cacheReadTokens: 0, cost: 0, savings: 0 }
  )

  return {
    totalInputTokens: totals.inputTokens,
    totalOutputTokens: totals.outputTokens,
    totalCacheReadTokens: totals.cacheReadTokens,
    totalCost: totals.cost,
    totalSavings: totals.savings,
    averageCostPerGeneration: totals.cost / metrics.length
  }
}

/**
 * Logger les m√©triques pour monitoring
 */
export function logGenerationMetrics(
  endpoint: string,
  userId: string,
  metrics: GenerationMetrics
): void {
  if (process.env.NODE_ENV === 'development') {
    console.log('üìä Generation Metrics:', {
      endpoint,
      userId: userId.substring(0, 8),
      inputTokens: metrics.inputTokens,
      outputTokens: metrics.outputTokens,
      cacheReadTokens: metrics.cacheReadTokens || 0,
      totalCost: `$${metrics.totalCost.toFixed(4)}`,
      estimatedSavings: `$${metrics.estimatedSavings?.toFixed(4) || '0'}`,
      savingsPercentage: metrics.estimatedSavings
        ? `${((metrics.estimatedSavings / (metrics.totalCost + metrics.estimatedSavings)) * 100).toFixed(1)}%`
        : '0%'
    })
  }

  // TODO: Envoyer vers syst√®me de monitoring (Sentry, Datadog, etc.)
}

// Export des types
export type { Anthropic }
